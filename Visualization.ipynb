{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-88113bbc8d3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcycler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcycler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrcsetup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\colorbar.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0martist\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmartist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollections\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m from .transforms import (Bbox, IdentityTransform, Transform, TransformedBbox,\n\u001b[0;32m     15\u001b[0m                          TransformedPatchPath, TransformedPath)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data files\n",
    "df = pd.read_csv('dengue_features_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first divide training data into categorical and numerical subsets\n",
    "X_train_cat = X_train.select_dtypes(include = 'category').copy()\n",
    "X_train_num = X_train.select_dtypes(include = 'number').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split x and y\n",
    "x = df.iloc[:,df_train_sj.columns != 'total_cases']\n",
    "y = df.iloc[:,df_train_sj.columns == 'total_cases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seasonal_decompose\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "result = seasonal_decompose(df_train_sj['total_cases'], model='additive',freq=1)\n",
    "result.plot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "mod1 = ARIMA(endog=df_train_sj_y,exog=df_train_sj_x, order=(2, 0, 0))\n",
    "res1 = mod.fit()\n",
    "df_test_sj_y = res1.forecast(steps=259, exog=df_test_sj_x, alpha=0.05)\n",
    "df_test_sj_y = pd.DataFrame(data=df_test_sj_y[0])\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dickey_Fuller_test function\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def Dickey_Fuller_test(df):\n",
    "    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    for y in df.columns:\n",
    "        if df[y].dtype in numeric_dtypes:\n",
    "            X = df[y].values\n",
    "            result = adfuller(X)\n",
    "            print('\\033[91m','\\033[1m','Test Result for:',y,'\\033[0m','\\033[90m')\n",
    "            print('ADF Statistic: %f' % result[0])\n",
    "            print('p-value: %f' % result[1])\n",
    "            print('Critical Values:')\n",
    "            for key, value in result[4].items():\n",
    "                print('\\t%s: %.3f' % (key, value))\n",
    "            print(\"-----------------------------------------------------------------\")\n",
    "            \n",
    "#Dickey_Fuller_test(df_train_sj)                  \n",
    "#p-value <= 0.05 , stationary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auto Arima\n",
    "\n",
    "#load the data\n",
    "data = df_Sales\n",
    "\n",
    "#divide into train and validation set\n",
    "train = data[:int(0.7*(len(data)))]\n",
    "valid = data[int(0.7*(len(data))):]\n",
    "\n",
    "#preprocessing (since arima takes univariate series as input)\n",
    "#train.drop('Month',axis=1,inplace=True)\n",
    "#valid.drop('Month',axis=1,inplace=True)\n",
    "\n",
    "#plotting the data\n",
    "train['Sales'].plot()\n",
    "valid['Sales'].plot()\n",
    "\n",
    "#building the model\n",
    "from pmdarima import auto_arima\n",
    "#model = auto_arima(train, trace=True, error_action='ignore', suppress_warnings=True)\n",
    "\n",
    "model =  auto_arima(train,start_p=0, d=1, start_q=0, \n",
    "                          max_p=5, max_d=5, max_q=5, start_P=0, \n",
    "                          D=1, start_Q=0, max_P=5, max_D=5,\n",
    "                          max_Q=5, m=12, seasonal=True, \n",
    "                          error_action='warn',trace = True,\n",
    "                          supress_warnings=True,stepwise = True,\n",
    "                          random_state=20,n_fits = 50 )\n",
    "model.fit(train)\n",
    "\n",
    "forecast = model.predict(n_periods=len(valid))\n",
    "forecast = pd.DataFrame(forecast,index = valid.index,columns=['Prediction'])\n",
    "\n",
    "#plot the predictions for validation set\n",
    "plt.plot(train, label='Train')\n",
    "plt.plot(valid, label='Valid')\n",
    "plt.plot(forecast, label='Prediction')\n",
    "plt.show()\n",
    "\n",
    "#calculate rmse\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rms = sqrt(mean_squared_error(valid,forecast))\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe to CSV\n",
    "df.to_csv(\"df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop col\n",
    "df.drop(['col_name'], axis = 1,inplace=True) \n",
    "\n",
    "#Drop Rows with condition\n",
    "df.drop(df[df['col_name'] > 250].index, inplace = True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Value (Time Series)\n",
    "df = df.iloc[1:]\n",
    "df = df.fillna(method='ffill')\n",
    "df['colname'].fillna('stringvalue', inplace = True)\n",
    "df['colname'].fillna(df['colname'].median(), inplace = True)  #Median\n",
    "df['colname'].fillna(df['colname'].mode()[0],inplace=True)    #Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing\n",
    "df_test = df_test.set_index('week_start_date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing Data Types\n",
    "\n",
    "#to datetime\n",
    "df['week_start_date']=df_f_train['week_start_date'].apply(pd.to_datetime)\n",
    "\n",
    "#to category\n",
    "df[\"city\"] = df_f_train[\"city\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot Graph of each attribute vs target value\n",
    "\n",
    "def PlotAttributeVsTarget(data_train,target):\n",
    "    for y in data_train.columns:\n",
    "        if y!= target:\n",
    "            plt.figure(figsize=(12,5))\n",
    "            sns.scatterplot(x=data_train[y],y=data_train[target])\n",
    "    \n",
    "#PlotAttributeVsTarget(df_train,'total_cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap\n",
    "\n",
    "plt.figure(figsize = (12, 9))\n",
    "s = sns.heatmap(df.corr(),\n",
    "               annot = True, \n",
    "               cmap = 'RdBu',\n",
    "               vmin = -1, \n",
    "               vmax = 1)\n",
    "s.set_yticklabels(s.get_yticklabels(), rotation = 0, fontsize = 12)\n",
    "s.set_xticklabels(s.get_xticklabels(), rotation = 90, fontsize = 12)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Control Plot Seaborn\n",
    "\n",
    "plt.figure(figsize = (16,6))\n",
    "s=sns.countplot(x=\"Downloads\",hue=\"Category\", data=df)\n",
    "s.set_yticklabels(s.get_yticklabels(), rotation = 0, fontsize = 12)\n",
    "s.set_xticklabels(s.get_xticklabels(), rotation = 90, fontsize = 12)\n",
    "s.set_xlabel(\"Downloads\",fontsize=16)\n",
    "s.set_ylabel(\"Count\",fontsize=16)\n",
    "s.axes.set_title(\"Count Plot\",fontsize=30)\n",
    "\n",
    "plt.setp(s.get_legend().get_texts(), fontsize='16') # for legend text\n",
    "plt.setp(s.get_legend().get_title(), fontsize='20') # for legend title\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sweetviz\n",
    "import sweetviz as sv\n",
    "\n",
    "my_report = sv.analyze(df)\n",
    "my_report.show_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autoviz\n",
    "\n",
    "from autoviz.AutoViz_Class import AutoViz_Class\n",
    "\n",
    "AV = AutoViz_Class()\n",
    "\n",
    "sep = ','\n",
    "dft = AV.AutoViz(filename=\"\",sep=sep, depVar='V30', dfte=shop_df, header=0, verbose=2, \n",
    "                 lowess=False, chart_format='svg', max_rows_analyzed=150000, max_cols_analyzed=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Value indicator column addition\n",
    "cols=list(df.columns) \n",
    "cols.remove('col_to_be_romoved')\n",
    "df_1 = df[cols].isnull().astype(int).add_suffix('_missing_value_indicator')\n",
    "df = pd.concat([df.reset_index(drop=True),df_1.reset_index(drop=True)], axis=1)\n",
    "\n",
    "#Method 2\n",
    "df = df.assign(Flag_col_name = df.col_name.notnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN Imputer for missing value\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "imputed_df_train_test = pd.DataFrame(imputer.fit_transform(df_train_test))\n",
    "imputed_df_train_test.columns = df_train_test.columns\n",
    "imputed_df_train_test.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from missingpy import MissForest\n",
    "imputer = MissForest()\n",
    "X_imputed = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fancyimpute import MICE\n",
    "\n",
    "trans = MICE()\n",
    "trans.complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Data Visualization\n",
    "\n",
    "import missingno as mno\n",
    "mno.matrix(df, figsize = (20, 6))\n",
    "mno.heatmap(df)\n",
    "mno.bar(df)\n",
    "\n",
    "#Linear Reg for missing data\n",
    "#https://www.kaggle.com/shashankasubrahmanya/missing-data-imputation-using-regression\n",
    "#https://www.kaggle.com/residentmario/simple-techniques-for-missing-data-imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable Importance\n",
    "\n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from yellowbrick.features import FeatureImportances\n",
    "\n",
    "# clf = DecisionTreeClassifier()\n",
    "# viz = FeatureImportances(clf)\n",
    "# viz.fit(X_sample, y_sample)\n",
    "# viz.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "x=cross_val_score(model, X_train, y_train, cv=10, scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skewness in dataset\n",
    "from scipy.stats import skew\n",
    "\n",
    "numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerics2 = []\n",
    "for i in data_train_test.columns:\n",
    "    if data_train_test[i].dtype in numeric_dtypes: \n",
    "        numerics2.append(i)\n",
    "\n",
    "skew_features = data_train_test[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "skews = pd.DataFrame({'skew':skew_features})\n",
    "skews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Skewness\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "\n",
    "high_skew = skew_features[skew_features > 0.5]\n",
    "high_skew = high_skew\n",
    "skew_index = high_skew.index\n",
    "\n",
    "for i in skew_index:\n",
    "    data_train_test[i]= boxcox1p(data_train_test[i], boxcox_normmax(data_train_test[i]+1))\n",
    "\n",
    "        \n",
    "skew_features2 = data_train_test[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "skews2 = pd.DataFrame({'skew':skew_features2})\n",
    "skews2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library for VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calc_vif(X):\n",
    "\n",
    "    # Calculating VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    return(vif)\n",
    "\n",
    "calc_vif(df.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get dummies Hot Encoding\n",
    "print(df.shape)\n",
    "df = pd.get_dummies(df,drop_first = True).reset_index(drop=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df, prefix=['A', 'D'], columns=['A', 'D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get dummies Hot Encoding\n",
    "df = pd.get_dummies(df,drop_first = True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add dfs\n",
    "print('Shape Of train df:',data_train.shape)\n",
    "print('Shape Of test df:',data_test.shape)\n",
    "data_train_test = pd.concat([data_train, data_test]) \n",
    "print('Shape Of train test  df:',data_train_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Percentage of missing Values present\n",
    "round(data.isnull().sum()/len(data)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing data\n",
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coeffiecinet Of Variation\n",
    "#CV is a measure of relative variation or dispersion \n",
    "def CoeffiecinetOfVariation(df):\n",
    "    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    numerics2 = []\n",
    "    for y in df.columns:\n",
    "        if df[y].dtype in numeric_dtypes:\n",
    "            numerics2.append(y)\n",
    "    SD_features = df[numerics2].apply(lambda x: np.std(x))\n",
    "    Mean_features = df[numerics2].apply(lambda x: np.mean(x))\n",
    "    CV_features = df[numerics2].apply(lambda x: (np.std(x)/np.mean(x)))\n",
    "    CV = pd.DataFrame({'SD':SD_features,'Mean':Mean_features,'CV':CV_features})\n",
    "    CV = CV.sort_values(by='CV',ascending=False)\n",
    "    return CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation\n",
    "#Correlation of each varibale with target\n",
    "def Correlation(df,target):\n",
    "    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    numerics2 = []\n",
    "    crr = {}\n",
    "    for y in df.columns:\n",
    "        if df[y].dtype in numeric_dtypes:\n",
    "            numerics2.append(y)\n",
    "            for col in numerics2:\n",
    "                if col != target:\n",
    "                    crr[col] = np.corrcoef(df[col],df[target])[1,0]\n",
    "                    \n",
    "    Crr_df = pd.DataFrame([crr])\n",
    "    Crr_df = Crr_df.T\n",
    "    Crr_df.columns = ['CorrCoef']\n",
    "    Crr_df = Crr_df.sort_values(by='CorrCoef',ascending=False)\n",
    "    return Crr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df, n=5):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]\n",
    "\n",
    "print(\"Top Absolute Correlations\")\n",
    "print(get_top_abs_correlations(df, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization and Standardlization\n",
    "\n",
    "#Standardlization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def Scaler(x):\n",
    "    scaler = StandardScaler().fit(x)\n",
    "    scaled_x = pd.DataFrame(scaler.transform(x))\n",
    "    scaled_x.columns = x.columns\n",
    "    return scaled_x\n",
    "\n",
    "colnames_list = ['A','B']\n",
    "\n",
    "def NormalizeCols(df,colnames_list):\n",
    "    for i in df.columns:\n",
    "        if i in colnames_list: \n",
    "            df[i] = (df[i] - df[i].min())/ (df[i].max()-df[i].min())      #Normalization\n",
    "    return df\n",
    "\n",
    "def StandardlizeCols(df,colnames_list):\n",
    "    for i in df.columns:\n",
    "        if i in colnames_list: \n",
    "            df[i] = (df[i] - df[i].mean())/ df[i].std()                    #Standardlization\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalisation required as Kmeans is distance based algorithm\n",
    "scaler = StandardScaler()\n",
    "df_C_scaled = scaler.fit_transform(df_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data        \n",
    "df_train = data_train_test.iloc[:1458,:] #if you remeber we had 1458 rows in train dataframe\n",
    "df_test = data_train_test.iloc[1458:,:]          \n",
    "print('Shape of Train df:', df_train.shape,'\\nShape of Test df:', df_test.shape, '\\nShape of train_test df:', data_train_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign X and Y\n",
    "X_train = df_train.iloc[:,df_train.columns != 'SalePrice']\n",
    "y_train = df_train.iloc[:,df_test.columns == 'SalePrice']\n",
    "X_test = df_test.iloc[:,df_test.columns != 'SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBOOST\n",
    "#XGBRegressor\n",
    "model_xgb = xgb.XGBRegressor()\n",
    "\n",
    "model_GB.fit(X_train,y_train.values.ravel())\n",
    "y_pred_model_xgb = model_xgb.predict(X_test)\n",
    "data_submit['Target'] = y_pred_model_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search #XGBOOST\n",
    "parameters = {\n",
    "    'colsample_bytree':[0.3,0.5,0.6],\n",
    "    'gamma':[0],\n",
    "    'min_child_weight':[.5,1.5,2.5],\n",
    "    'learning_rate':[0.1,0.2,0.3],\n",
    "    'max_depth':[4,5,6],\n",
    "    'n_estimators':[1000,10000,100000],\n",
    "    'reg_alpha':[.1,.01,.001],\n",
    "    'reg_lambda':[.1,.01,.001],\n",
    "    'subsample':[0.5,0.6,0.7]  \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = model, param_grid = parameters, n_jobs=-1,iid=False, verbose=10,scoring='neg_mean_squared_error')\n",
    "grid_search.fit(train_x,train_y)\n",
    "print('best params')\n",
    "print (grid_search.best_params_)\n",
    "print('best score')\n",
    "print (grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search\n",
    "#GradientBoostingRegressor\n",
    "parameters = {\n",
    "    'n_estimators':[20],\n",
    "    'learning_rate':[0.1],\n",
    "    'max_depth':[6],\n",
    "    'max_features':['sqrt'],\n",
    "    'min_samples_leaf':[15],\n",
    "    'min_samples_split':[2],\n",
    "    'loss': ['huber']   \n",
    "}\n",
    "grid_search = GridSearchCV(estimator = model_GB, param_grid = parameters, n_jobs=-1,iid=False, verbose=10,scoring='neg_mean_absolute_error')\n",
    "grid_search.fit(X_train,y_train)\n",
    "print('best params')\n",
    "print (grid_search.best_params_)\n",
    "print('best score')\n",
    "print (grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting Regression \n",
    "model_GB = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "\n",
    "model_GB.fit(X_train,y_train.values.ravel())\n",
    "y_pred_model_GB = model_GB.predict(X_test)\n",
    "data_submit['Target'] = y_pred_model_GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LightGBM :\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "\n",
    "model_lgb.fit(X_train,y_train.values.ravel())\n",
    "y_pred_model_lgb = model_lgb.predict(X_test)\n",
    "data_submit['Target'] = y_pred_model_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#from kneed import KneeLocator\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "kmeans_kwargs = {\"init\": \"random\",\"n_init\": 10,\"max_iter\": 300,\"random_state\": 42,}\n",
    "   \n",
    "# A list holds the SSE values for each k\n",
    "sse = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(df_C)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    \n",
    "\n",
    "#https://realpython.com/k-means-clustering-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(1, 11), sse)\n",
    "plt.xticks(range(1, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list holds the silhouette coefficients for each k\n",
    "silhouette_coefficients = []\n",
    "\n",
    "# Notice you start at 2 clusters for silhouette coefficient\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(scaled_features)\n",
    "    score = silhouette_score(scaled_features, kmeans.labels_)\n",
    "    silhouette_coefficients.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(2, 11), silhouette_coefficients)\n",
    "plt.xticks(range(2, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.decomposition import PCA #Principal Component Analysis\n",
    "\n",
    "#plotly imports\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "\n",
    "plotX=frame\n",
    "\n",
    "#PCA with two principal components\n",
    "pca_2d = PCA(n_components=2)\n",
    "\n",
    "#This DataFrame contains the two principal components that will be used for the 2-D visualization mentioned above\n",
    "PCs_2d = pd.DataFrame(pca_2d.fit_transform(plotX.drop([\"Cluster\"], axis=1)))\n",
    "PCs_2d.columns = [\"PC1_2d\", \"PC2_2d\"]\n",
    "\n",
    "plotX = pd.concat([plotX,PCs_2d], axis=1, join='inner')\n",
    "\n",
    "cluster0 = plotX[plotX[\"Cluster\"] == 0]\n",
    "cluster1 = plotX[plotX[\"Cluster\"] == 1]\n",
    "cluster2 = plotX[plotX[\"Cluster\"] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instructions for building the 2-D plot\n",
    "\n",
    "#trace1 is for 'Cluster 0'\n",
    "trace1 = go.Scatter(\n",
    "                    x = cluster0[\"PC1_2d\"],\n",
    "                    y = cluster0[\"PC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 0\",\n",
    "                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace2 is for 'Cluster 1'\n",
    "trace2 = go.Scatter(\n",
    "                    x = cluster1[\"PC1_2d\"],\n",
    "                    y = cluster1[\"PC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 1\",\n",
    "                    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace3 is for 'Cluster 2'\n",
    "trace3 = go.Scatter(\n",
    "                    x = cluster2[\"PC1_2d\"],\n",
    "                    y = cluster2[\"PC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 2\",\n",
    "                    marker = dict(color = 'rgba(0, 255, 200, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "data = [trace1, trace2, trace3]\n",
    "\n",
    "title = \"Visualizing Clusters in Two Dimensions Using PCA\"\n",
    "\n",
    "layout = dict(title = title,\n",
    "              xaxis= dict(title= 'PC1',ticklen= 5,zeroline= False),\n",
    "              yaxis= dict(title= 'PC2',ticklen= 5,zeroline= False)\n",
    "             )\n",
    "\n",
    "fig = dict(data = data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write CSV\n",
    "df.to_csv('df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Chi square test\n",
    "\n",
    "from scipy.stats import chi2_contingency \n",
    "\n",
    "def Chi_Square_Test(df,variable1,variable2,alpha):\n",
    "    print(variable1,\"Vs\",variable2)\n",
    "    contingency_df = pd.crosstab(df[variable1], df[variable2], margins = False) \n",
    "    stat, p, dof, expected = chi2_contingency(contingency_df) \n",
    "    print(\"p value is \",p) \n",
    "    if p <= alpha: \n",
    "        print('Dependent (reject H0)') \n",
    "    else: \n",
    "        print('Independent (H0 holds true)') \n",
    "    print(\"-----------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "results=stats.f_oneway(df['Discount Percentage'][df['Wishlisted'] == 'YES'],df['Discount Percentage'][df['Wishlisted'] == 'NO'])\n",
    "\n",
    "# interpret p-value \n",
    "alpha = 0.05\n",
    "print(\"p value is \", results.pvalue) \n",
    "if p <= alpha: \n",
    "    print('Dependent (reject H0)') \n",
    "else: \n",
    "    print('Independent (H0 holds true)') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "model = ols('age ~ race',                 # Model formula\n",
    "            data = voter_frame).fit()\n",
    "                \n",
    "anova_result = sm.stats.anova_lm(model, typ=2)\n",
    "print (anova_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HeatMap for Mixed Datatype\n",
    "from dython.nominal import associations\n",
    "associations(data)\n",
    "#associations(df.drop(['published_time'], 1),figsize=(20,20))\n",
    "\n",
    "#Pearson's R for continuous-continuous cases\n",
    "#Correlation Ratio for categorical-continuous cases\n",
    "#Cramer's V/Theil's U for categorical-categorical cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Visualization\n",
    "#SweetViz\n",
    "\n",
    "import sweetviz as sv\n",
    "\n",
    "my_report = sv.analyze(my_dataframe)\n",
    "my_report.show_html() # Default arguments will generate to \"SWEETVIZ_REPORT.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pivot table\n",
    "\n",
    "df=data.copy()\n",
    "df_discount=df[(df['Topic Covered']=='music production')|(df['Topic Covered']=='A.I.')|(df['Topic Covered']=='R')]\n",
    "df_disc = df_discount[(df_discount['Is Paid']==True)&(df_discount['Certification Provided']=='YES')]\n",
    "table=pd.pivot_table(df_disc,values='Discount Percentage', index='Topic Covered', columns='Course Provider', aggfunc=np.mean)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn imports\n",
    "from sklearn.decomposition import PCA #Principal Component Analysis\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "\n",
    "#This DataFrame contains the two principal components that will be used for the 2-D visualization mentioned above\n",
    "X_PCA = pd.DataFrame(pca.fit_transform(X.drop(['V2','V11','V12','V13','V15','V16','V17'], axis=1)))\n",
    "\n",
    "print(pca.explained_variance_ratio_.sum())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression Score\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,mean_squared_log_error,median_absolute_error,r2_score\n",
    "#mean_absolute_percentage_error,\n",
    "\n",
    "class color:\n",
    "    BLUE = '\\033[94m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'\n",
    "    \n",
    "def Display_Scores(y_valid,prediction):\n",
    "    Round_upto=5\n",
    "    print(color.BOLD + color.BLUE+ 'mean_absolute_error: ' + color.END, mean_absolute_error(y_valid, prediction).round(Round_upto))\n",
    "    print(color.BOLD + color.BLUE+ 'mean_squared_error: ' + color.END, mean_squared_error(y_valid, prediction).round(Round_upto))\n",
    "    print(color.BOLD + color.BLUE+ 'mean_squared_log_error: ' + color.END, mean_squared_log_error(y_valid, prediction).round(Round_upto))\n",
    "    #print(color.BOLD + color.BLUE+ 'mean_absolute_percentage_error: ' + color.END, mean_absolute_percentage_error(y_valid, prediction).round(Round_upto))\n",
    "    print(color.BOLD + color.BLUE+ 'median_absolute_error: ' + color.END, median_absolute_error(y_valid, prediction).round(Round_upto))\n",
    "    print(color.BOLD + color.RED+ 'r2_score: ' + color.END, r2_score(y_valid, prediction).round(Round_upto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification\n",
    "#Method to display all classification scores\n",
    "\n",
    "from sklearn.metrics import roc_curve,confusion_matrix,accuracy_score,precision_score, recall_score,f1_score,classification_report,roc_auc_score\n",
    "\n",
    "\n",
    "class color:\n",
    "    BLUE = '\\033[94m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'\n",
    "\n",
    "def Display_Scores(y_valid,prediction):\n",
    "    Round_upto=3\n",
    "    print(color.BOLD + color.RED+ 'Confusion Matrix: ' + color.END) \n",
    "    print(confusion_matrix(y_valid, prediction))\n",
    "    print(color.BOLD + color.BLUE+ 'Accuracy: ' + color.END, accuracy_score(y_valid, prediction).round(Round_upto))\n",
    "    print(color.BOLD + color.BLUE+ 'Precision: ' + color.END, precision_score(y_valid, prediction).round(Round_upto))\n",
    "    print(color.BOLD + color.BLUE+ 'Recall: ' + color.END, recall_score(y_valid, prediction).round(Round_upto))\n",
    "    print(color.BOLD + color.BLUE+ 'f1 Score: ' + color.END, f1_score(y_valid, prediction).round(Round_upto))\n",
    "    #print(color.BOLD + color.BLUE+ 'Roc auc score: ' + color.END, roc_auc_score(y_valid, prediction).round(Round_upto))\n",
    "    print(color.BOLD + color.RED+ 'Classification Report: ' + color.END)\n",
    "    print(classification_report(y_valid, prediction))\n",
    "    \n",
    "def Plot_ROC_Curve(y_valid,prediction):\n",
    "    print(color.BOLD + color.RED+ 'ROC Curve: ' + color.END) \n",
    "    fpr, tpr, thresholds = roc_curve(y_valid, prediction)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot(fpr, fpr, linestyle = '--', color = 'k')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Confusion Matrix\n",
    "\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(pipeline, X_valid, y_valid,cmap='Blues')  \n",
    "plt.title(\"Confusion Matrix\",fontdict = {'fontsize' : 20}) \n",
    "plt.rcParams.update({'font.size': 24})\n",
    "plt.grid(None)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier_detection\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "outlier_detection = DBSCAN(eps = .2, metric = 'euclidean', min_samples = 5,n_jobs = -1)\n",
    "clusters = outlier_detection.fit_predict(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z score outlier detection\n",
    "def DetectOultier(df,colname):\n",
    "    data=df[colname]\n",
    "    mean = np.mean(data) \n",
    "    sd = np.std(data)\n",
    "    threshold = 5\n",
    "    outliers = []\n",
    "    for i in data: \n",
    "        z = (i-mean)/sd \n",
    "        if abs(z) > threshold:  \n",
    "            outliers.append(i)\n",
    "            \n",
    "    print(\"The detected outliers for variable \", colname ,\" are: \", outliers)\n",
    "    print(\"-------------------------------------------------------------------------------------\")\n",
    "    \n",
    "for i in Cols:\n",
    "    DetectOultier(X_num,i)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
